{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a Linear Regression problem with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are ready to implement our own linear regression example. \n",
    "\n",
    "In linear regression, our hypothesis function $h_\\theta$ is:\n",
    "\n",
    "$$h_\\theta(x) = \\theta_0 + \\theta_1x$$\n",
    "\n",
    "And, as we are doing regression, our cost function is: \n",
    "\n",
    "$$J(\\theta_0,\\theta_1) = \\frac{1}{m}\\sum_{i=1}^m(\\hat{y}_i-y_i)^2 = \\frac{1}{m}\\sum_{i=1}^m(h_\\theta(x_i)-y_i)^2 $$\n",
    "\n",
    "Nota that, the cost funtion is just the sum of all the square errors from our hypothesis ($\\hat{y}_i$) versus the data ($y_i$).\n",
    "\n",
    "The best parameters for our hypothesis will give us the **minimum cost function**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding a minimum for J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding a minimum of a function is equivalent to finding the parameters that make the gradient of that function to vanish. In other words:\n",
    "\n",
    "$$\\nabla_{\\theta} J(\\theta) = 0$$\n",
    "\n",
    "We will implement two ways of solving this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A) Gradient descent (Numerical method)\n",
    "\n",
    "\n",
    "From a starting point ($\\theta$), we will try to move to a new point $\\theta '$, decreasing the cost funtion ($J(\\theta)$). We will do this many times, up to we find a minimum (or close enough to it).\n",
    "\n",
    "#### Partial differentials of the cost function (using chain rule)\n",
    "\n",
    "$$\\frac{\\partial}{\\partial\\theta_0} = \\frac{2}{m}\\sum_{i=1}^m(h_\\theta(x_i)-y_i)$$\n",
    "\n",
    "$$\\frac{\\partial}{\\partial\\theta_1} = \\frac{2}{m}\\sum_{i=1}^m(h_\\theta(x_i)-y_i) \\cdot x_i$$\n",
    "\n",
    "Finally, we need to update iteratively the values for $\\theta_0$ and $\\theta_1$. Using Gradient Descent algorithm  with learning rate ($\\alpha$) until convergence criterion ($\\epsilon$) is achieved:\n",
    "\n",
    "         while (convergence==False):\n",
    "$$\\theta_0' = \\theta_0 - \\alpha \\frac{\\partial J}{\\partial\\theta_0} $$\n",
    "$$\\theta_1' = \\theta_1 - \\alpha \\frac{\\partial J}{\\partial\\theta_1} $$\n",
    "$$J' = J(\\theta_0',\\theta_1')$$\n",
    "$$\\Delta J = abs(J'-J)$$\n",
    "$$ convergence = \\Delta J < \\epsilon)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B) Normal equations (Algebra)\n",
    "\n",
    "In matrix notation, we can implement our hypothesis as:\n",
    "\n",
    "$$h_\\theta (x^{(i)})=(x^{(i)})^T \\theta$$\n",
    "\n",
    "Note that, in this case, if we want to consider our hypothesis such $h(\\theta) = \\theta_0 + \\theta_i x^{(i)}$ where $x$ is a vector, for consistency, we need to introduce an additional \"constant feature\" in our data. In other words, we need to map our input data as follows:\n",
    "\n",
    "$$x_i \\rightarrow [1,x_i]$$\n",
    "\n",
    "we can express gradient of J as follows:\n",
    "\n",
    "$$\\nabla_{\\theta} J(\\theta) = X^T X \\theta - X^T \\vec{y}$$\n",
    "\n",
    "To minimize J, we set its derivatives to zero, therefore obtaining the **normal equations**:\n",
    "\n",
    "$$ X^T X \\theta = X^T \\vec{y}$$\n",
    "\n",
    "We can solve this equation for theta.\n",
    "\n",
    "As a final remark, we can extend this method to non linear hypothesis by extending our input data $x$ to the features we need. For example, for a parabolic fit:\n",
    "\n",
    "$$x_i \\rightarrow [1,x_i,x_i^2]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem\n",
    "\n",
    "Giving the data provided below (x->y), find the best equation fit, using:\n",
    "\n",
    "* Gradient Descent\n",
    "* Normal Equations\n",
    "\n",
    "Using your own python implementation, using numpy and scipy tools (**not scipy!**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as scio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "theta_0 = 2\n",
    "theta_1 = 5\n",
    "\n",
    "X = (np.random.randn(100) + 1) * 50\n",
    "jitter = 50 * np.random.randn(100)\n",
    "y = theta_0 + theta_1 * X + jitter\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "ax.scatter(X, y)\n",
    "ax.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
